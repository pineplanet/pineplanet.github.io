---
title: TIL 2022-01-31 python 딥러닝
tags: [TIL, python]
categories: TIL
---

- 머신러닝의 이해 
  - 복잡한 데이터(정형데이터) 를 입력 받아서 최적의 함수를 찾아 주는 것 
  - 러닝 종류 
    - 기호주의 
    - 연결주의 <- 딥러닝
      - 크게 확장됨 
    - 확률 주의 
    - 유전 알고리즘 
    - 유추 주의 
  - 머신러닝 대두 : 
    - 소프트웨어에 기대되는 복잡도가 높아지면서 사람의 인지 능력과 비슷할 정도의 구현이 필요해짐 
    - 사람의 코딩(로직)으로 대응하기에는 예외가 많아서 어려움 
  - 머신 학습 방법 
    - 예측 오류를 보정할 수 있도록 최적화하는 알고리즘으로 학습
- 딥러닝 개요
  - 비정형 데이터 분야에서 활발하고 빠르게 발전 중! 
  - 인간 뇌 학습, 기억 매커니즘 모방(뉴런!)
    - 뇌: 자극 -> 화학 물질 -> 연결 -> 반복할수록 연결이 강해짐 
    - 자극 임계점을 넘지 못하면 연결 x 반응 하지 않음 
    - 이러한 연결을 통한 학습 강화 => 딥러닝 
      - input 
      - input nodes layer
      - hidden nodes layer
      - output nodes layer 
      - output
  - 머신은 어떻게 학습 하는가? 
    - 심층 신경망을 구축 -> 최적의 가중치(파라미터)를 찾아냄 -> 최적의 함수 만들기 
- 딥러닝 장단점, 특징 
  - 머신러닝 VS 딥러닝 
    - 장 : 딥러닝은 매우 유연, 확장 가능
      - CNN Feature Extractor : feature 를 뽑아내는과정을 모델링 안에 넣음
        - 참고: feature engineering은 스스로 못함.
      - 과거에는 최적화 하기 어려운 문제를 모델링 할 수 있음 
    - 단 : 연산이 매우 많이 필요, 고차원 최적화 -> 정형 데이터에서는 큰 성능 향상 어려움(가성비? ) 
  - 인간 VS 딥러닝 : 인식 방법이 다름 ! 
    - 인간은 universal 하게 대상을 인식 하지만, 딥러닝은 학습된 모델 을 새로운 것들을 인식 시킬 때 예측 어려울 수 있음
    - 인간과 딥러닝이 인식을 잘 할 수 있는 영역이 다름 -> 협업! 
- 퍼셉트론(Perceptron)
  - 가장 단순한 형태의 신경망 
    - Hidden layer 가 없음 
    - 입력 feature , 가중치, Unit Step(activation) , 출력 값(0,1)으로 구성 
    - sigmoid : 값들을 확률 값(0,1) 으로 변환하는 함수 
    - 방법: 
      - 입력값 * 가중치 -> 모두 더한 weighted sum -> activation 함수 (unit step or sigmoid ) 등 적용 -> 선형 회귀 구현 
      - F(x)의 최적화된 w 값 찾기! -> 값과 실제 값 차이가 최소가 되는 weight 값을 찾음 
        - 어떻계? 경사하강법으로 예측, 출력 값의 차이를 줄이는 방향성으로 w 를 계속 변경 
  - 경사 하강법 
    - 회귀(Regression) 개요
      - 회귀: 하나의 종속 변수와 여러 독립 변수(종속변수에 영향을 줄 수 있는 요소,feature) 간 상관 관계 모델링 
      - 최적의 회귀 계수(w) 찾아내기 
      - 단순 선형 회귀 : 독립변수를 1개만 두는 것 
      - 회귀 오류 측정 Residual Sum of Square : 오류 값의 제곱을 구해서 더하는 방식 
        - 제곱을 쓰는 이유: 오류는 + , - 값 모두 있을 수 있는데 그냥 합으로 하면 값 반영 어려움, 절대값으로 하면 미분 어려움
        - w 변수가 중심 변수임 
        - MSE(비용) : 제곱 합을 학습데이터의 건수로 나눈 것 -> 비용함수의 값을 지속해서 감소 시켜 더이상 오류값이 감소하지 않는 최소 오류 값을 찾는다. 
  - 비용 최소화 하기 
    - 고차원 방정식 으로 도출 : 
      - 문제점 : w 파라미터의 갯수가 많을 경우에는 해결 하기 어렵다. 
    - 경사하강법은 보다 간단, 직관적인 비용함수 최소화 솔루션을 제공
        - 점진적인 하강! -> 점진적으로 반복적 계산 -> 파라미터 업데이트
    - 어떻게 하면!!!! 오류가 작아지는 방향으로 w 값을 보정할 수 있을까? 
      - 미분! : 미분은 증가 또는 감소의 방향성을 나타낸다. 
      - 미분된 1차 함수 기울기가 최소인점 == 비용함수가 최소인 지점 
      - Loss(w) 는 가중치 (w1, w2 그리고 w0 등)을 가지고 있어서 일반적인 미분 이 아닌 , w 변수에 편미분함 
      - 이 편미분 값을 기존 가중치에 계속 업데이트 함 
        - 직전의 w 값에 (학습률*편미분 값을 빼줌 == 감소 시키는 크기 == 스텝) 
        - 정해진 iteration 만큼 편미분 값 구하고 -> weight, bias 업데이트 
    - 보스턴 주택 가격 실습 
      1. 직접 만들어보기(?)
        - 데이터
        - Weight와 Bias의 Update 값을 계산하는 함수 생성.
        - Gradient Descent 를 적용하는 함수 생성
        - 정규화/표준화 작업
          - MinMaxScaler(sklearn)
      2. keras
        - keras Sequential,
        - Dense layer 사용 
        - compile : adam optimizer 사용, lose - mse , 성능 - mse 사용 
        - fit 학습 : 학습데이터, target, epochs
  - 확률적 경사하강법(Stochastic Gradient Descent)와 미니 배치(Mini-batch) 경사하강법
    - Gradient Descent 방식의 문제
      - 학습 데이터가 많고, 입력 데이터가 클 수록 Gradient Descent 를 계산 하는데 자원이 많이 소모됨 
      - GPU를 사용하더라도 메모리 부족으로 연산 불가능할 경우가 있음 
      - 그래서 Stochastic Gradient Descent 방식이 도입됨 
    - Stochastic Gradient Descent 
      - 전체 데이터 중에 임 의로 1건만 선택해서 계산
    - Mini-batch Gradient Descent (<- 일반적으로 사용)
      - 전체 데이터 중 특정 크기의 데이터만 선택해서 GD 계산
    - 실습
      - SGD
        - SGD 기반으로 Weight/Bias update 값 구하기
          - 1건만 가져온다. Weight와 Bias의 Update 값을 계산
        - st_gradient_descent
          - target -> 랜덤한 인덱스 1개 추출 -> rm, lstat 데이터 ->Weight/Bias update 값 구하는 함수 호출 
        - 데이터 셋을 scale 변경 한 후 gradent_desecnt
      - Mini-batch Gradient Descent 
        - 케라스는 무조건 mini batch 로 수행함 
        - batch size 를 정하지 않으면 32를 할당함 
    - 경사하강법 == 딥러닝의 뼈대 ! 
      - 이슈 
        - Learning Rate 크기(스텝) 이슈 
          - 너무 작으면: 최소점에 수렴하기 까지 오래걸림 , 자원을 너무 많이 써버림 
          - 너무 크면: 최소점을 못찾고 넘어가버림 -> 발산이 되버릴 수도 있음 
        - 전역 최소점과 국소 최소점 이슈 
          - 낮은 위치가 여러개 있을 때 local minimum 에서 빠져 나오지 못해 global minimum을 찾지 못하는 경우
          - feature 가 많고 매우 많은 파라미터 를 가진 함수 의 경우 
