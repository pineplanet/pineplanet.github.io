---
title: TIL 2022-01-21 python 머신러닝 
tags: [TIL, python]
categories: TIL
---

# 오늘 배운 것 

- colab 과 pycharm 연동하기
  - 연동은 성공, jupyter 는 안됨. 
  - 연결 끊기면 세팅하는 거의 모든 과정을 다시 해야함 
  - 그냥 코랩 쓰는걸로..? 
- 머신러닝 
  - 복습 
    - 데이터 전처리에서 중요한 것 : 
      - 스케일 조정 ( 대표적 방법: 표준점수(z 점수) - (특성 - 평균) / 표준편차 )
  - 1 k-최근접 이웃 회귀 ▶️회귀 문제를 이해하고 k-최근접 이웃 알고리즘으로 풀어 보기
    - 회귀(regression) : 티겟이 임의의 숫자가 된다. 
      - 골턴(19세기 통계,사회학자) : 키가 큰 사람의 아이 -> 평균을 향해 감
      - 지도 학습 알고리즘 
        - 분류 
        - 회귀 <- 오늘 배울 것
    - k-최근접 이웃 회귀
      - 분류인 경우에는 주변 이웃의 특성 투표, 
      - 회귀의 경우에는 주변 이웃의 평균 
    - 데이터 준비
      - 길이 , (높이, 두께)  -> 무게
      - 회귀 문제에서는 stratify 를 사용하지 않음 
      - 사이킷런이 기대하는 데이터로 모양 바꾸기 
    - 결정계수(R2)
      - 예측이 타겟의 평균 정도 라면 분모, 분자가 비슷해져서 0에 가까워짐 -> 값은 1에 가까워짐 
      - 예측이 타겟을 정확히 맞춘다면 1에 가까워짐 -> 값은 0에 가까워짐 
      - R2 은 0~1 사이의 값인데, 1에 가까울 수록 높은 예측이 됨 
    - 과대적합 vs 과소적합
      - 일반적으로 train score 가 test score 보다 높아야 함 
      - 과소 적합 : train < test
        - 이웃의 갯수를 높이면 과소 적합이 됨 
        - 이웃 개수 줄이기 
        - 복잡도가 낮다 
      - 과대 적합 : train >>> test 
        - 이웃의 갯수를 낮추면 과대 적합이 됨 
        - 복잡도가 높다.
      - 하이퍼 파라미터 : 이웃의 개수 처럼 조정(직접 세팅) 해서 결과를 바꿀 수 있는 변수
  - 2 선형 회귀 ▶️ 사이킷런으로 선형 회귀 모델 만들어 보기
    - k-최근접 이웃의 한계
      - 훈련세트 범위 밖의 데이터를 예측 하기 어렵다. 
      - 길이 가 길어지면 무게도 같이 올라가는 경우 예측 모델을 만드려면? 
    - 선형 회귀(ax+b)
      - 로지스틱 회귀, 신경망의 기초! 
      - 특성 1개만 대상으로 할 경우 1차원 직선의 방정식을 찾아냄 (여러 특성을 반영하는 것도 당연 가능)
      - 모델 객체 _ : 모델이 학습한 값
        - coef_ :기울기
          - 넘파이 배열, 여러 특성을 사용할 경우 특성마다 여러 계수가 계산되서 저장됨
        - intercept_: y 절편
    - 다항 회귀(ax^2 + bx + c )
      - 선형모델의 단점 : 아주 작은 샘플을 예측 한다면 ? 음수의 결과가 나올 수 있다. y 절편이 음수로 나오는 경우 
        - 곡선 형태로 모델을 만들 수 있다면? 
      - 2차 방정식 형태로 예측 함
      - 사이킷런에 클래스가 따로 있지는 않고, x2 배열을 추가해서 모델링 
  - 3  특성 공학과 규제 ▶️특성 공학과 규제 알아보기
    - 다중 회귀(multiple(multinomial) regression)
      - 여러 특성을 함께 학습 시키는 것 
      - 특성 공학(Feature Engineering;) :
        - 특성들을 더하거나, 빼고, 기존의 특성들을 조합해 새로운 특성을 만들어서 추가 하는 등 
        - 머신러닝 을 위해 특성들을 가공 하는 것 

    - 데이터 준비
    - 사이킷런의 변환기 Transformer(PolynomialFeatures)
      - 다항 특성 만들기 
           - 메소드
             - fit(학습 ㄴㄴ) : 특성 파악
             - transform : 예 [[2,3]] -> [[1. 2. 3. 4. 6. 9.]]
             - fit-transform (fit + transform)
        
    - 다중 회귀 모델 훈련하기
      - 훈련 세트 보다 특성이 더 많아지는 등 너무 복잡성이 커지게 되면? 
        - 과대 적합 이 된다. train 셋은 거의 완벽하게 맞출 수는 있어도, test를 맞추기 어려워짐 
          - 과대 적합을 완화 하는 방법은? 규제!
      
    - 규제
      - 가중치(=기울기)를 작게 만든다. 
      - 가중치가 높으면 모델에 벌을 준다!(무슨벌..?!)
      - LinearRegression 은 원래 스케일을 조정 하지 않는다. 그런데 규제할때는 표준화를 해야함. 왜냐하면 가중치에 따라 벌을 줘야 하는데, 가중치를 판단 할 때 공평 해야하니까 
      - 평균, 표준편차를 이용하기 때문에 train fit한 객체로 test 도 transform 해야함
    
    - 릿지 회귀 (L2 규제)
      - 가중치의 제곱을 벌칙으로 사용함 
      - 규제 강도 : alpha 매개변수 (default = 1) 크게 하면 강도가 세지고, 작게 하면 강도가 약해짐 
        - 어느 정도가 제일 좋은지 확인 해보자! 
          - alpha_list = [0.001,0.01,0.1,1,10,100] 을 만들어서 for 문으로 값을 하나씩 넣어 score 를 plot 으로 그려본다. 
            - alpha_list = [0.001,0.01,0.1,1,10,100] 은 관례 상 전체적으로 보기 위해 넣는 범위 
            - plot 을 그릴 때는 그래프를 한눈에 직관적으로 보기 위해서 x축을 np.log10(alpha_list) 으로 통일(?) 해서 그린다. 이렇게 하면 수가 작은 앞부분도 뒷부분과 같은 비율로 볼 수 있으니까 
            
    - 라쏘 회귀(L1 규제)
      - 가중치의 절대 값을 벌칙으로 사용함 
- 알고리즘 
  - 리트코드 문제 5개 풀이 
  