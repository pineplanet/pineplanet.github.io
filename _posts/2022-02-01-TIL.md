---
title: TIL 2022-02-01 python 딥러닝
tags: [TIL, python]
categories: TIL
---
# 오늘 배운 것

- 심층신경망의 이해와 오차 역전파(Backpropagation) 개요
  - 단일 퍼셉트론 은 간단한 문제만 해결 가능
  - 심층신경망 -> AI 빙하기 부숨입력층, 은닉층, 출력층으로 구성
  - 각 은닉층의 결과 -> sigmoid or Relu activation 통과 -> 다음 은닉층
  - 최종 예측 식 -> 경사하강법 어떻게???(너무나 복잡한 식 답없음, 개별 가중치 미분 하기 어려움 ) => Backpropagation
    - Feed Forward : output 값 에 로스 함수 선택 ->
    - Backpropagation : 출력 부터 역순 으로 직전 은닉층 으로 부터 입력층까지 거꾸로 가면서 weight update
    - 위의 과정을 반복 수행
    - 미분의 연쇄 법칙 ...
- 오차 역전파(Backpropagation)의 이해 - 미분의 연쇄 법칙
  - 미분 : x가 증가할 때 y는 얼마나 증가할까?
    - 일반미분
    - 편미분
      - 여러 변수가 있는 함수에서 한 함수에만 주목, 나머지 변수들의 값은 고정 시키고 주목한 함수로만 미분 하는 것
  - 미분의 연쇄 법칙( 합성 함수의 미분)
    - 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱
    - 복잡 한 고차원 식을 간편하게 미분 하게 해줌!
      - 양파 처럼 바깥 함수를 먼저 미분 하고 -> 안쪽 함수를 다시 미분 해준다
    - 해당 함수가 내포함수의 연속적인 결합이라면 연쇄 법칙으로 쉽게 미분 가능
      - f(g(h(x))) -> df/dx = df/dz * dz/dy * dy/dx
- 합성 함수의 연쇄 결합이 적용된 심층 신경망
  - 인풋 -> 아웃풋1 -> 아웃풋2 -> 최종 아웃풋 까지 거쳐가면서 앞의 결과물이 뒤의 함수의 인풋이 된다. 
  - Backpropagation
    - Upstream Gradient : 아웃풋에서 거꾸로 올라오는 미분 값
    - Local Gradient : 각 뉴런에서 계산 한 값
    - Upstream Gradient에 Local Gradient 를 곱해서 Gradient 를 구할 수 있음 
    - 구하려고 하는 w 는 기여한 모든 output 의 path를(upstream, local 미분 값들을 곱 ) 거쳐서 연쇄 미분 해서 합 해야 함 
